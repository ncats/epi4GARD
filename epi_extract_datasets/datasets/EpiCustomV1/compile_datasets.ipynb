{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2934f94",
   "metadata": {},
   "source": [
    "# Compiling all of the datasets into one training dataset\n",
    "\n",
    "As of this point custom labeled train, val, & test sets were created using ruled-based and statistical tagging w/IOB method\n",
    " - Disease B-DIS\n",
    " - Location B-LOC\n",
    " - Incidence/Prevalence B-EPI (tagged in case of relation extraction later on)\n",
    " - Statistics B-STAT\n",
    " \n",
    "### Additionally\n",
    "- The val set will remain unchanged\n",
    "- The test set will be modified by manual curation  \n",
    "\n",
    "There exists the MISC datasets {CoNLL(PP) location dataset, NCBI-disease, BC5CDR-disease, (i2b2-disease?)}  \n",
    "All of these have train/val/test (and train_dev for BC5CDR) sets  \n",
    "Given that my goal is to validate and test identification of Disease | Location | Incidence/Prevalence | Statistics on a realistic dataset, I will be combining the val/test/train_dev sets into the training data for the MISC datasets and only be doing val and test with my custom dataset, thus\n",
    "\n",
    "### Goals:\n",
    "1. Read in CoNLL(PP) for location\n",
    "2. Read in NCBI-disease\n",
    "3. Read in BC5CDR-disease\n",
    "3. Read in the custom dataset\n",
    "4. Combine them into a training set\n",
    "5. Save in a way that the tokenizer can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608b3e5",
   "metadata": {},
   "source": [
    "## (1) CoNLL(PP)-Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843a587d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seqeval in /home/wzkariampuzha/.local/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/wzkariampuzha/.local/lib/python3.6/site-packages (from seqeval) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/wzkariampuzha/.local/lib/python3.6/site-packages (from seqeval) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/wzkariampuzha/.local/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/wzkariampuzha/.local/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/wzkariampuzha/.local/lib/python3.6/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conllpp (/home/wzkariampuzha/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install transformers\n",
    "#!{sys.executable} -m pip install datasets\n",
    "#!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install seqeval\n",
    "\n",
    "from datasets import load_dataset\n",
    "coNLL = load_dataset(\"conllpp\")\n",
    "coNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4291a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER_tag '5' is B-LOC, '6' is I-LOC\n",
    "#Get numbers on the amount of location \n",
    "\n",
    "def read_loc_dataset(dataset):\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for sentence in dataset:\n",
    "        #Only add sentences that actually have location tags (i.e. meaningfully annotated sentences)\n",
    "        if (5 in sentence['ner_tags'] or 6 in sentence['ner_tags']):\n",
    "            tags = []\n",
    "            #Only keep location tags\n",
    "            for tag in sentence['ner_tags']:\n",
    "                label = 'O'\n",
    "                if tag ==5:\n",
    "                    label = 'B-LOC'\n",
    "                if tag == 6:\n",
    "                    label = 'I-LOC'\n",
    "                tags.append(label)\n",
    "            \n",
    "            #Raise error if mismatch\n",
    "            if len(sentence['tokens']) != len(tags):\n",
    "                print('mismatch')\n",
    "                print(sentence['tokens'])\n",
    "                print(tags)\n",
    "            \n",
    "            token_docs.append(sentence['tokens'])\n",
    "            tag_docs.append(tags)\n",
    "        \n",
    "    return token_docs, tag_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50151c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_loc, train_tags_loc = read_loc_dataset(coNLL[\"train\"])\n",
    "val_texts_loc, val_tags_loc = read_loc_dataset(coNLL[\"validation\"])\n",
    "test_texts_loc, test_tags_loc = read_loc_dataset(coNLL[\"test\"])\n",
    "#Combine \n",
    "#loc_tokens = [train_texts_loc, val_texts_loc, test_texts_loc]\n",
    "#loc_tags = [train_tags_loc,test_tags_loc,test_tags_loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f725b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUSSELS B-LOC\n",
      "1996-08-22 O\n",
      "\n",
      "Germany B-LOC\n",
      "'s O\n",
      "representative O\n",
      "to O\n",
      "the O\n",
      "European O\n",
      "Union O\n",
      "'s O\n",
      "veterinary O\n",
      "committee O\n",
      "Werner O\n",
      "Zwingmann O\n",
      "said O\n",
      "on O\n",
      "Wednesday O\n",
      "consumers O\n",
      "should O\n",
      "buy O\n",
      "sheepmeat O\n",
      "from O\n",
      "countries O\n",
      "other O\n",
      "than O\n",
      "Britain B-LOC\n",
      "until O\n",
      "the O\n",
      "scientific O\n",
      "advice O\n",
      "was O\n",
      "clearer O\n",
      ". O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "for i in range(2): #for sentence in abstract\n",
    "    for j in range(len((train_texts_loc[i]))): #for token in sentence\n",
    "        print(train_texts_loc[i][j], train_tags_loc[i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eaa791",
   "metadata": {},
   "source": [
    "## (2) NCBI-disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658d2572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ncbi_disease (/home/wzkariampuzha/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 5433\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 924\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 941\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncbi_dz = load_dataset(\"ncbi_disease\")\n",
    "ncbi_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5deb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER_tag '1' is B-DIS, '2' is I-DIS\n",
    "#https://github.com/huggingface/datasets/tree/master/datasets/ncbi_disease\n",
    "\n",
    "def read_dis_dataset(dataset):\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for sentence in dataset:\n",
    "        tags = []\n",
    "        for tag in sentence['ner_tags']:\n",
    "            label = 'O'\n",
    "            if tag ==1:\n",
    "                label = 'B-DIS'\n",
    "            if tag == 2:\n",
    "                label = 'I-DIS'\n",
    "            tags.append(label)\n",
    "            \n",
    "        #Raise error if mismatch\n",
    "        if len(sentence['tokens']) != len(tags):\n",
    "            print('mismatch')\n",
    "            print(len(sentence['tokens']))\n",
    "            print(len(tags))\n",
    "        else:\n",
    "            token_docs.append(sentence['tokens'])\n",
    "            tag_docs.append(tags)\n",
    "        \n",
    "    return token_docs, tag_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be24cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_dis, train_tags_dis = read_dis_dataset(ncbi_dz[\"train\"])\n",
    "val_texts_dis, val_tags_dis = read_dis_dataset(ncbi_dz[\"validation\"])\n",
    "test_texts_dis, test_tags_dis = read_dis_dataset(ncbi_dz[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be346871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification O\n",
      "of O\n",
      "\n",
      "The O\n",
      "adenomatous B-DIS\n",
      "polyposis I-DIS\n",
      "coli I-DIS\n",
      "( I-DIS\n",
      "APC I-DIS\n",
      ") I-DIS\n",
      "tumour I-DIS\n",
      "- O\n",
      "suppressor O\n",
      "protein O\n",
      "controls O\n",
      "the O\n",
      "Wnt O\n",
      "signalling O\n",
      "pathway O\n",
      "by O\n",
      "forming O\n",
      "a O\n",
      "complex O\n",
      "with O\n",
      "glycogen O\n",
      "synthase O\n",
      "kinase O\n",
      "3beta O\n",
      "( O\n",
      "GSK O\n",
      "- O\n",
      "3beta O\n",
      ") O\n",
      ", O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "for i in range(2): #for sentence in abstract\n",
    "    for j in range(len((train_texts_loc[i]))): #for token in sentence\n",
    "        print(train_texts_dis[i][j], train_tags_dis[i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0d530",
   "metadata": {},
   "source": [
    "## (3) BC5CDR-disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f2f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what the .tsv file looks like, can remove later\n",
    "files = {'train':\"./datasets/NER/BC5CDR-disease/train.tsv\", \n",
    "         'test':\"./datasets/NER/BC5CDR-disease/test.tsv\",\n",
    "         'train_dev':\"./datasets/NER/BC5CDR-disease/train_dev.tsv\",\n",
    "         'devel':\"./datasets/NER/BC5CDR-disease/devel.tsv\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261c4f8",
   "metadata": {},
   "source": [
    "Since these datasets are not split into different abstracts, only sentences, i am going to make a list out of the four sets so that the hierarchy is the same as the other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2544d3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with train\n",
      "Done with test\n",
      "Done with train_dev\n",
      "Done with devel\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "BC5CDR_tokens,BC5CDR_tags = [],[]\n",
    "for key,value in files.items():\n",
    "    with open(value,'r') as file:\n",
    "        reader = csv.reader(file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "        sentence_tokens, sentences_tags=[],[]\n",
    "        for row in reader:\n",
    "            if len(row)%2==0:\n",
    "                if len(row)==0:\n",
    "                    BC5CDR_tokens.append(sentence_tokens.copy())\n",
    "                    BC5CDR_tags.append(sentences_tags.copy())\n",
    "                    sentence_tokens.clear()\n",
    "                    sentences_tags.clear()\n",
    "                else:\n",
    "                    sentence_tokens.append(row[0])\n",
    "                    if row[1]=='I':\n",
    "                        sentences_tags.append('I-DIS')\n",
    "                    elif row[1]=='B':\n",
    "                        sentences_tags.append('B-DIS')\n",
    "                    else:\n",
    "                        sentences_tags.append('O')\n",
    "            else: \n",
    "                print('bad row',row)\n",
    "    file.close()\n",
    "    print('Done with',key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7fefc65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selegiline O\n",
      "- O\n",
      "induced O\n",
      "postural B-DIS\n",
      "hypotension I-DIS\n",
      "in O\n",
      "Parkinson B-DIS\n",
      "' I-DIS\n",
      "s I-DIS\n",
      "disease I-DIS\n",
      "\n",
      "OBJECTIVES O\n",
      ": O\n",
      "The O\n",
      "United O\n",
      "Kingdom O\n",
      "Parkinson B-DIS\n",
      "' I-DIS\n",
      "s I-DIS\n",
      "Disease I-DIS\n",
      "Research O\n",
      "Group O\n",
      "( O\n",
      "UKPDRG O\n",
      ") O\n",
      "trial O\n",
      "found O\n",
      "an O\n",
      "increased O\n",
      "mortality O\n",
      "in O\n",
      "patients O\n",
      "with O\n",
      "Parkinson B-DIS\n",
      "' I-DIS\n",
      "s I-DIS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "for i in range(2): #for sentence in abstract\n",
    "    for j in range(int(len(BC5CDR_tokens[i])/2)): #for token in sentence\n",
    "        print(BC5CDR_tokens[i][j], BC5CDR_tags[i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e75e9",
   "metadata": {},
   "source": [
    "## (4) Read custom training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "091c4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "epi_train_abstracts, epi_train_labels= [],[]\n",
    "with open('epi_train_set.tsv','r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    sentence_tokens,sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                epi_train_abstracts.append(sentence_tokens.copy())\n",
    "                epi_train_labels.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2074ac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aims O\n",
      "Coeliac B-DIS\n",
      "disease I-DIS\n",
      "CD B-DIS\n",
      "is O\n",
      "an O\n",
      "autoimmune B-DIS\n",
      "disorder I-DIS\n",
      "with O\n",
      "a O\n",
      "\n",
      "Growth O\n",
      "retardation O\n",
      "GR O\n",
      "generally O\n",
      "accompanies O\n",
      "CD B-DIS\n",
      "due O\n",
      "to O\n",
      "gastrointestinal O\n",
      "complications O\n",
      "and O\n",
      "should O\n",
      "be O\n",
      "treated O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "for i in range(2): #for sentence in abstract\n",
    "    for j in range(int(len(epi_train_abstracts[i])/2)): #for token in sentence\n",
    "        print(epi_train_abstracts[i][j], epi_train_labels[i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8641cd",
   "metadata": {},
   "source": [
    "## (5) Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c51ae043",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_texts_loc + train_texts_dis + val_texts_loc + val_texts_dis + test_texts_loc + test_texts_dis +BC5CDR_tokens + epi_train_abstracts \n",
    "train_tags = train_tags_loc + train_tags_dis + val_tags_loc + val_tags_dis + test_tags_loc + test_tags_dis + BC5CDR_tags + epi_train_labels\n",
    "\n",
    "#train_texts = [train_texts_loc, train_texts_dis, val_texts_loc, val_texts_dis, test_texts_loc, test_texts_dis, BC5CDR_tokens, epi_train_abstracts]\n",
    "#train_tags = [train_tags_loc, train_tags_dis, val_tags_loc, val_tags_dis, test_tags_loc, test_tags_dis, BC5CDR_tags, epi_train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34f301d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42803 42803\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts),len(train_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d9a14",
   "metadata": {},
   "source": [
    "## (6) Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbfd9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n"
     ]
    }
   ],
   "source": [
    "with open('training_set.tsv', \"w\") as f:\n",
    "    for i in range(len(train_texts)): #For sentence in abstract\n",
    "        for j in range(len(train_texts[i])): #for token in sentence\n",
    "            output = str(train_texts[i][j]) +'\\t' +str(train_tags[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10881392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#IF use the dataset list instead of concatentation\n",
    "'''\n",
    "with open('training_set.tsv', \"w\") as f:\n",
    "    for s in range(len(train_texts)): #for set in training sets\n",
    "        for i in range(len(train_texts[s])): #For sentence in set\n",
    "            for j in range(len(train_texts[s][i])): #for token in sentence\n",
    "                output = str(train_texts[s][i][j]) +'\\t' +str(train_tags[s][i][j])+'\\n'\n",
    "                f.write(output)\n",
    "            f.write('\\n')\n",
    "            if i%500==0:\n",
    "                print(i)\n",
    "f.close()\n",
    "'''\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5dda8",
   "metadata": {},
   "source": [
    "## Testing adding in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05755957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training data\n",
    "import csv\n",
    "train_texts, train_tags= [],[]\n",
    "with open('training_set.tsv','r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    sentence_tokens, sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                train_texts.append(sentence_tokens.copy())\n",
    "                train_tags.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "861e0dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUSSELS B-LOC\n",
      "\n",
      "Germany B-LOC\n",
      "'s O\n",
      "representative O\n",
      "to O\n",
      "the O\n",
      "European O\n",
      "Union O\n",
      "'s O\n",
      "veterinary O\n",
      "committee O\n",
      "Werner O\n",
      "Zwingmann O\n",
      "said O\n",
      "on O\n",
      "Wednesday O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "for i in range(2): #for sentence in abstract\n",
    "    for j in range(int(len(train_texts[i])/2)): #for token in sentence\n",
    "        print(train_texts[i][j], train_tags[i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7f28231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the validation data\n",
    "import csv\n",
    "val_texts, val_tags= [],[]\n",
    "with open('epi_val_set.tsv','r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    sentence_tokens, sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                val_texts.append(sentence_tokens.copy())\n",
    "                val_tags.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ab7950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aim O\n",
      "To O\n",
      "evaluate O\n",
      "the O\n",
      "risk O\n",
      "factors O\n",
      "and O\n",
      "incidence B-EPI\n",
      "of O\n",
      "Asherman B-DIS\n",
      "Syndrome I-DIS\n",
      "\n",
      "Methods O\n",
      "A O\n",
      "total O\n",
      "of O\n",
      "2546 O\n",
      "patients O\n",
      "who O\n",
      "had O\n",
      "surgical O\n",
      "abortion O\n",
      "uterine O\n",
      "evacuation O\n",
      "and O\n",
      "curettage O\n",
      "before O\n",
      "the O\n",
      "20th O\n",
      "gestational O\n",
      "week O\n",
      "with O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "for i in range(2): #for sentence in abstract\n",
    "    for j in range(int(len(val_texts[i])/2)): #for token in sentence\n",
    "        print(val_texts[i][j], val_tags[i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49f9101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the testing data\n",
    "import csv\n",
    "test_texts, test_tags= [],[]\n",
    "with open('epi_test_set.tsv','r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    sentence_tokens, sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                test_texts.append(sentence_tokens.copy())\n",
    "                test_tags.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3db345d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background O\n",
      "Infection O\n",
      "with O\n",
      "Entamoeba O\n",
      "histolytica O\n",
      "and O\n",
      "associated O\n",
      "\n",
      "The O\n",
      "overall O\n",
      "low O\n",
      "prevalence B-EPI\n",
      "in O\n",
      "the O\n",
      "Western O\n",
      "world O\n",
      "as O\n",
      "well O\n",
      "as O\n",
      "the O\n",
      "possibly O\n",
      "prolonged O\n",
      "latency O\n",
      "period O\n",
      "between O\n",
      "infection B-DIS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "for i in range(2): #for sentence in abstract\n",
    "    for j in range(int(len(test_texts[i])/2)): #for token in sentence\n",
    "        print(test_texts[i][j], test_tags[i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c7f9fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-DIS', 'B-EPI', 'B-STAT', 'I-EPI', 'O', 'I-LOC', 'B-LOC', 'B-DIS', 'I-STAT'}\n",
      "{'I-DIS': 0, 'B-EPI': 1, 'B-STAT': 2, 'I-EPI': 3, 'O': 4, 'I-LOC': 5, 'B-LOC': 6, 'B-DIS': 7, 'I-STAT': 8}\n",
      "{0: 'I-DIS', 1: 'B-EPI', 2: 'B-STAT', 3: 'I-EPI', 4: 'O', 5: 'I-LOC', 6: 'B-LOC', 7: 'B-DIS', 8: 'I-STAT'}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = set(tag for doc in (train_tags+val_tags+test_tags) for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "print(unique_tags)\n",
    "print(tag2id)\n",
    "print(id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2160e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "#Modified\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-large-cased-v1.1')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "test_encodings = tokenizer(test_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab63497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_tags(tags, encodings, which_set):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    i=0\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        '''\n",
    "        print('doc_labels')\n",
    "        print(doc_labels)\n",
    "        print('')\n",
    "        \n",
    "        print('doc_offset')\n",
    "        print(doc_offset)\n",
    "        print('')\n",
    "        '''\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        '''\n",
    "        print('doc_enc_labels')\n",
    "        print(doc_enc_labels)\n",
    "        print('')\n",
    "        \n",
    "        print('arr_offset')\n",
    "        print(arr_offset)\n",
    "        print('')\n",
    "        '''\n",
    "        if (np.count_nonzero((arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)) != len(doc_labels)):\n",
    "            print(np.count_nonzero((arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)))\n",
    "            print(len(doc_labels))\n",
    "            if which_set =='train':\n",
    "                train_texts.pop(i)\n",
    "                train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "            if which_set =='val':\n",
    "                val_texts.pop(i)\n",
    "                val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "            if which_set =='test':\n",
    "                test_texts.pop(i)\n",
    "                test_encodings = tokenizer(test_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "            print('-----------------------------------')\n",
    "        else:\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "            encoded_labels.append(doc_enc_labels.tolist())\n",
    "        i+=1\n",
    "        \n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31ef79d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "23\n",
      "-----------------------------------\n",
      "42802 42802\n"
     ]
    }
   ],
   "source": [
    "train_labels = encode_tags(train_tags, train_encodings,'train')\n",
    "print(len(train_labels), len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dd27306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1185 1185\n"
     ]
    }
   ],
   "source": [
    "val_labels = encode_tags(val_tags, val_encodings,'val')\n",
    "print(len(val_labels), len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b07157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454 454\n"
     ]
    }
   ],
   "source": [
    "test_labels = encode_tags(test_tags, test_encodings,'test')\n",
    "print(len(test_labels), len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5ce7fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Format_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "test_encodings.pop(\"offset_mapping\")\n",
    "\n",
    "train_dataset = Format_Dataset(train_encodings, train_labels)\n",
    "val_dataset = Format_Dataset(val_encodings, val_labels)\n",
    "test_dataset = Format_Dataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79f04df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-large-cased-v1.1', num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6310231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified\n",
    "from transformers import EvalPrediction\n",
    "from torch import nn\n",
    "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray): # -> Tuple[List[int], List[int]]\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        #print('preds.shape',preds.shape)\n",
    "        batch_size, seq_len = preds.shape\n",
    "        \n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(id2tag[label_ids[i][j]])\n",
    "                    preds_list[i].append(id2tag[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "def compute_metrics(p: EvalPrediction): #-> Dict\n",
    "    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "    return {\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "111ffb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results',          # output directory\n",
    "    overwrite_output_dir = True,\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=f'./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    seed = 1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # compute metric defined above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a9d3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "#Seed is a helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if installed).\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bef3d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 42802\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2676\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwzkariampuzha\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./results</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wzkariampuzha/huggingface\" target=\"_blank\">https://wandb.ai/wzkariampuzha/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wzkariampuzha/huggingface/runs/3936uhxe\" target=\"_blank\">https://wandb.ai/wzkariampuzha/huggingface/runs/3936uhxe</a><br/>\n",
       "                Run data is saved locally in <code>/home/wzkariampuzha/test/wandb/run-20210803_015555-3936uhxe</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='2676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  25/2676 09:26 < 18:08:43, 0.04 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.097700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f360a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "# For convenience, we also re-save the tokenizer to the same directory,\n",
    "# so that you can share your model easily on huggingface.co/models =)\n",
    "if trainer.is_world_process_zero():\n",
    "    tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained('./results/', num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "import os\n",
    "results = {}\n",
    "result = trainer.evaluate()\n",
    "output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        for key, value in result.items():\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "            results.update(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a7baf",
   "metadata": {},
   "source": [
    "Testing & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "predictions, label_ids, metrics = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed16fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align Predictions\n",
    "preds_list, _ = align_predictions(predictions, label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prediction results\n",
    "\n",
    "output_test_results_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_test_results_file, \"w\") as writer:\n",
    "        for key, value in metrics.items():\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save actual predictions\n",
    "\n",
    "output_test_predictions_file = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_test_predictions_file, \"w\") as writer:\n",
    "        i = 0\n",
    "        for sentence in test_texts:\n",
    "            j=0\n",
    "            for token in sentence:\n",
    "                output = token +'\\t' +preds_list[i][j]+'\\n'\n",
    "                writer.write(output)\n",
    "                j+=1\n",
    "            i+=1\n",
    "''''''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
